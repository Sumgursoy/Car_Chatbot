"""
Vision Module â€” Crawl4AI + Gemini Vision
==========================================
KullanÄ±cÄ±nÄ±n verdiÄŸi ilan URL'sindeki fotoÄŸraflarÄ± Crawl4AI ile Ã§eker,
Gemini 2.0 Flash Vision ile analiz eder.

AkÄ±ÅŸ:
  1. Crawl4AI â†’ URL'ye git, screenshot + gÃ¶rselleri topla
  2. GÃ¶rselleri base64'e Ã§evir
  3. Gemini Vision'a gÃ¶nder â†’ detaylÄ± TÃ¼rkÃ§e analiz al
"""

import os
import asyncio
import base64
import json
import httpx
from logger import get_logger

log = get_logger("vision")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CRAWL4AI Ä°LE GÃ–RSEL TOPLAMA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def crawl_listing_images(url: str) -> dict:
    """
    Verilen ilan URL'sine Crawl4AI ile gidip gÃ¶rselleri ve screenshot'u toplar.

    Returns:
        {
            "screenshot_b64": str | None,
            "images_b64": list[str],
            "image_urls": list[str],
            "page_title": str,
            "page_text": str       # ilan aÃ§Ä±klama metni (markdown)
        }
    """
    from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode

    log.info(f"Crawl baÅŸlatÄ±lÄ±yor: {url}")

    run_config = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS,
        screenshot=True,
        screenshot_wait_for=2.0,
        wait_for_images=True,
        scan_full_page=True,
        page_timeout=30000,
        verbose=False,
    )

    try:
        async with AsyncWebCrawler() as crawler:
            result = await crawler.arun(url=url, config=run_config)

            if not result.success:
                log.error(f"Crawl baÅŸarÄ±sÄ±z: {result.error_message}")
                return {
                    "hata": f"Sayfa yÃ¼klenemedi: {result.error_message}",
                    "screenshot_b64": None,
                    "images_b64": [],
                    "image_urls": [],
                    "page_title": "",
                    "page_text": "",
                }

            # Screenshot (base64)
            screenshot_b64 = result.screenshot if result.screenshot else None
            log.info(f"Screenshot: {'âœ…' if screenshot_b64 else 'âŒ'}")

            # Sayfa baÅŸlÄ±ÄŸÄ± ve metni
            page_title = ""
            if hasattr(result, 'metadata') and result.metadata:
                page_title = result.metadata.get("title", "")
            page_text = result.markdown_v2.raw_markdown if hasattr(result, 'markdown_v2') and result.markdown_v2 else (result.markdown or "")

            # GÃ¶rselleri topla
            all_images = result.media.get("images", []) if result.media else []
            log.info(f"Toplam {len(all_images)} gÃ¶rsel bulundu")

            # Kaliteli gÃ¶rselleri filtrele (score > 3, kÃ¼Ã§Ã¼k ikonlarÄ± atla)
            quality_images = []
            for img in all_images:
                src = img.get("src", "")
                score = img.get("score", 0)
                # KÃ¼Ã§Ã¼k ikonlarÄ±, logo'larÄ± ve placeholder'larÄ± atla
                if not src or "logo" in src.lower() or "icon" in src.lower():
                    continue
                if "placeholder" in src.lower() or "avatar" in src.lower():
                    continue
                if score is not None and score >= 2:
                    quality_images.append(img)
                elif score is None:
                    # Score yoksa da ekle
                    quality_images.append(img)

            # En iyi 5 gÃ¶rseli seÃ§ (skora gÃ¶re)
            quality_images.sort(key=lambda x: x.get("score", 0), reverse=True)
            selected = quality_images[:5]
            log.info(f"SeÃ§ilen gÃ¶rsel sayÄ±sÄ±: {len(selected)}")

            # GÃ¶rselleri indir ve base64'e Ã§evir
            image_urls = [img["src"] for img in selected]
            images_b64 = await _download_images_as_base64(image_urls)

            return {
                "screenshot_b64": screenshot_b64,
                "images_b64": images_b64,
                "image_urls": image_urls,
                "page_title": page_title,
                "page_text": page_text[:2000],  # Ä°lk 2000 karakter yeterli
            }

    except Exception as e:
        log.error(f"Crawl hatasÄ±: {e}")
        return {
            "hata": str(e),
            "screenshot_b64": None,
            "images_b64": [],
            "image_urls": [],
            "page_title": "",
            "page_text": "",
        }


async def _download_images_as_base64(urls: list[str]) -> list[str]:
    """GÃ¶rsel URL'lerini indirip base64 string olarak dÃ¶ner."""
    results = []
    async with httpx.AsyncClient(timeout=15.0, follow_redirects=True) as client:
        for url in urls:
            try:
                resp = await client.get(url)
                if resp.status_code == 200 and resp.headers.get("content-type", "").startswith("image"):
                    b64 = base64.b64encode(resp.content).decode("utf-8")
                    results.append(b64)
                    log.info(f"  âœ… Ä°ndirildi: {url[:80]}...")
                else:
                    log.warning(f"  âš ï¸ AtlandÄ± ({resp.status_code}): {url[:80]}...")
            except Exception as e:
                log.warning(f"  âŒ Ä°ndirme hatasÄ±: {e} â€” {url[:80]}")
    return results


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ GEMÄ°NÄ° VÄ°SÄ°ON ANALÄ°ZÄ° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def analyze_images_with_gemini(
    images_b64: list[str],
    screenshot_b64: str | None,
    page_text: str = "",
) -> str:
    """
    GÃ¶rselleri Gemini 2.0 Flash Vision'a gÃ¶nderip TÃ¼rkÃ§e analiz alÄ±r.
    """
    import google.generativeai as genai

    genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
    model = genai.GenerativeModel("gemini-2.0-flash")

    # Vision prompt
    prompt = f"""Sen bir araÃ§ uzmanÄ±sÄ±n. AÅŸaÄŸÄ±daki gÃ¶rseller bir araÃ§ ilanÄ±ndan alÄ±nmÄ±ÅŸtÄ±r.

GÃ¶rselleri dikkatle incele ve ÅŸu baÅŸlÄ±klarda detaylÄ± analiz yap:

## ğŸ“‹ Analiz BaÅŸlÄ±klarÄ±

1. **Genel Durum**: AracÄ±n genel gÃ¶rÃ¼nÃ¼mÃ¼, temizliÄŸi, bakÄ±mÄ±
2. **Boya ve Kaporta**: Boya durumu, Ã§izik, ezik, pas, boyalÄ±/deÄŸiÅŸen parÃ§a belirtileri
3. **Ä°Ã§ Mekan**: Koltuk durumu (aÅŸÄ±nma, yÄ±rtÄ±k, sigara yanÄ±ÄŸÄ±), gÃ¶sterge paneli, tavan dÃ¶ÅŸemesi
4. **Panel AralÄ±klarÄ±**: Panel boÅŸluklarÄ± simetrik mi? Kaza geÃ§miÅŸi belirtisi var mÄ±?
5. **Jant ve Lastikler**: Jant Ã§izikleri, lastik durumu, aÅŸÄ±nma
6. **TutarsÄ±zlÄ±k Tespiti**: Ä°lan metninde yazanlar ile fotoÄŸraflar arasÄ±nda Ã§eliÅŸki var mÄ±?

{f'## Ä°lan Metni (Referans):' if page_text else ''}
{page_text[:1000] if page_text else ''}

## Ã–nemli
- TÃ¼rkÃ§e analiz yap
- Net ve dÃ¼rÃ¼st ol, abartma ama gizleme de
- Emoji kullanarak okunabilirliÄŸi artÄ±r
- SonuÃ§ta 1-10 arasÄ± bir "GÃ¶rsel GÃ¼venilirlik Skoru" ver
"""

    # GÃ¶rselleri hazÄ±rla
    parts = [prompt]

    # Screenshot'u ekle
    if screenshot_b64:
        try:
            img_bytes = base64.b64decode(screenshot_b64)
            parts.append({
                "mime_type": "image/png",
                "data": img_bytes,
            })
        except Exception as e:
            log.warning(f"Screenshot decode hatasÄ±: {e}")

    # Galeri gÃ¶rsellerini ekle
    for i, img_b64 in enumerate(images_b64[:5]):
        try:
            img_bytes = base64.b64decode(img_b64)
            # MIME type tahmini (Ã§oÄŸu JPEG olacak)
            mime = "image/jpeg"
            if img_bytes[:4] == b'\x89PNG':
                mime = "image/png"
            elif img_bytes[:4] == b'RIFF':
                mime = "image/webp"
            parts.append({
                "mime_type": mime,
                "data": img_bytes,
            })
        except Exception as e:
            log.warning(f"GÃ¶rsel {i} decode hatasÄ±: {e}")

    if len(parts) < 2:
        return "âŒ Analiz edilecek gÃ¶rsel bulunamadÄ±."

    log.info(f"Gemini Vision'a {len(parts) - 1} gÃ¶rsel gÃ¶nderiliyor...")

    try:
        response = model.generate_content(parts)
        analysis = response.text
        log.info(f"Gemini analiz tamamlandÄ± ({len(analysis)} karakter)")
        return analysis
    except Exception as e:
        log.error(f"Gemini Vision hatasÄ±: {e}")
        return f"âŒ Gemini Vision analiz hatasÄ±: {str(e)}"


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ÃœST SEVÄ°YE FONKSÄ°YON â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def analyze_listing(url: str) -> dict:
    """
    Ãœst seviye fonksiyon: URL'den gÃ¶rselleri Ã§ek + Gemini Vision ile analiz et.

    Returns:
        {
            "url": str,
            "page_title": str,
            "gorsel_sayisi": int,
            "screenshot_b64": str | None,
            "analiz": str,
            "image_urls": list[str]
        }
    """
    log.info(f"=== Ä°lan GÃ¶rsel Analizi BaÅŸlÄ±yor: {url} ===")

    # 1. GÃ¶rselleri crawl et
    crawl_data = await crawl_listing_images(url)

    if "hata" in crawl_data and not crawl_data["images_b64"] and not crawl_data["screenshot_b64"]:
        return {
            "url": url,
            "hata": crawl_data["hata"],
            "gorsel_sayisi": 0,
            "analiz": f"âŒ Sayfa crawl edilemedi: {crawl_data['hata']}",
        }

    # 2. Gemini Vision ile analiz et
    analysis = await analyze_images_with_gemini(
        images_b64=crawl_data["images_b64"],
        screenshot_b64=crawl_data["screenshot_b64"],
        page_text=crawl_data["page_text"],
    )

    result = {
        "url": url,
        "page_title": crawl_data["page_title"],
        "gorsel_sayisi": len(crawl_data["images_b64"]),
        "screenshot_b64": crawl_data["screenshot_b64"],
        "analiz": analysis,
        "image_urls": crawl_data["image_urls"],
    }

    log.info(f"=== Analiz TamamlandÄ±: {len(crawl_data['images_b64'])} gÃ¶rsel ===")
    return result
